---
title: "Streamlit と Whisper で簡単議事録文字起こしアプリ作ってみた"
emoji: "👻"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["streamlit", "whisper", "openai", "chatgpt", "llm"]
published: false
---

## はじめに

皆さん、こんにちは！今日は、「音声ファイルから手軽に議事録を文字起こしできるアプリ」を作るというものです。

というのも、先日社内で 30 分程度の会議の録画動画を文字起こしする業務を頼まれたのですが、議事録の作成って、思った以上に時間がかかるんですよね。そこで簡単に文字起こしできる方法を調べて Google ドキュメントの音声入力機能を試してみたのですが、期待したほどの精度は得られず、結局 4 割くらいは手作業で行いました。こんな経験、いろんな人がしているんじゃないでしょうか？

![cover](https://storage.googleapis.com/zenn-user-upload/130dab6d8fc2-20231215.png)

そこで前々から気になっていた OpenAI の Whisper を使って、音声から文字起こしする機能作ってみたらどれくらい使えるんだろうと思い、今回の記事のテーマにしてみました。

この記事を通じて、議事録作成に苦労されている方々に、Whisper と Streamlit を使ったアプリ開発の魅力をお伝えできればと思います。手軽に試せる技術で日常の作業を効率化する楽しさを、皆さんにも感じていただけたら嬉しいです。

## Whisper（OpenAI API）とは

「Whisper」とは、OpenAI によって開発された革新的な音声認識技術です。この技術の最大の特徴は、その驚異的な精度と多言語対応能力にあります。Whisper は、日本語を含む多くの言語での音声をテキストに変換することが可能で、さまざまなアクセントや方言にも対応しています。

この技術は、特に音声をテキストに変換する際の正確性に優れており、バックグラウンドノイズが存在する環境下でも高い認識精度を保ちます。このため、会議やセミナーなど、さまざまな状況での議事録作成に非常に有効です。

Whisper を使用することで、従来の手作業に頼っていた議事録作成のプロセスを大幅に効率化できます。また、その多言語対応能力は、国際的なビジネスシーンにおいても大きな利点となり得ます。このように、Whisper は音声からテキストへの変換をシンプルかつ高効率に行うことを可能にします。

## Streamlit とは

次に紹介するのは「Streamlit」です。これは、Python で簡単かつ迅速に Web アプリケーションを開発できるオープンソースのフレームワークです。特に、データサイエンスや機械学習のプロジェクトにおいて、その効果を発揮します。

Streamlit の大きな魅力は、そのシンプルさにあります。複雑なフロントエンドの知識がなくても、わずかな Python コードでインタラクティブな Web アプリを作成できる点が特筆されます。データの可視化やユーザー入力の処理を簡単に組み込めるため、データ駆動型のアプリケーションを手早く構築できるのです。

さらに、Streamlit は迅速なプロトタイピングに最適です。短時間でアイデアを形にし、フィードバックを受けながら改善を重ねることが可能。この柔軟性は、開発のスピードと効率を大きく向上させます。

このプロジェクトでは、Whisper で得たテキストデータを利用し、Streamlit を通じて使いやすいアプリケーションに仕上げることを目指します。技術的な深い知識がなくても、Streamlit を用いれば、直感的かつ迅速にアプリを開発することができるのです。

## Whisper で文字起こししてみる

### ステップ 1: 事前準備

#### 必要なライブラリをインストール

```shell
pip install openai
pip install streamlit
pip install langchain
pip install python-dotenv
```

#### OpenAI API キーの取得

OpenAI のウェブサイトにアクセスし、API キーを取得します。OpenAI のアカウントを持っていない場合は、アカウントを作成し、API キーを生成する必要があります。API キーは、アプリケーションから OpenAI のサービスにアクセスするための重要な認証情報です。

#### 環境変数への API キーの設定

API キーは環境変数として保存することを推奨します。これにより、コード内に直接キーを記述するリスクを避けることができます。以下のコマンドを使用して、API キーを環境変数に設定します。

1. プロジェクトのルートディレクトリに .env ファイルを作成します。
2. .env ファイルに API キーを次の形式で保存します。

```shell
OPENAI_API_KEY=your_api_key_here
```

それでは、実際に手を動かしながら試してみましょう。ここでは、基本的な文字起こしのプロセスと、それを実行するためのサンプルコードをご紹介します

### ステップ 2: 音声ファイルの準備

まず、文字起こしを行いたい音声ファイルを準備します。これは会議の録音やインタビューなど、任意の音声を含むファイルで構いません。ファイルは.wav、.mp3 などの一般的な音声形式である必要があります。

今回はこちらの「あみたろの声素材工房」から、留守番電話の音声をお借りしました。

https://amitaro.net/voice/voice_dl/

### ステップ 3: 文字起こしの実行

次に、準備した音声ファイルを Whisper に送信し、文字起こしを行います。以下のコードは、音声ファイルを読み込み、テキストへの変換を行う簡単な例です。

https://platform.openai.com/docs/guides/speech-to-text/speech-to-text

```python:do_transcription.py
import os

from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()  # .env から環境変数を読み込む

# 環境変数からAPIキーを読み込む
api_key = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=api_key)

audio_file = open("../audio/rusuden_04.wav", "rb")
transcript = client.audio.transcriptions.create(
    model="whisper-1", file=audio_file, response_format="text"
)

print(transcript)
```

このコードを実行すると、指定した音声ファイルの内容がテキストに変換され、コンソールに出力されます。

```text
お電話ありがとうございます 大変申し訳ございませんが本日の営業はすべて終了させていただきました またのお電話をお待ちしております
```

この簡単なハンズオンを通じて、Whisper の強力な文字起こし機能を体験できます。
複雑な知識は不要で、サンプルコードを使ってすぐに音声認識をできるのが素晴らしいですね。

https://streamlit.io/

## Streamlit でアプリ化してみる

Whisper による文字起こし機能を持つアプリを、Streamlit を使って構築してみましょう。Streamlit は Python の知識があれば、直感的に Web アプリケーションを作成できる素晴らしいフレームワークです。

### ステップ 1: 基本的な Streamlit アプリの作成

Streamlit を使用して基本的な Web アプリケーションのスケルトンを作成します。
以下のサンプルコードは、簡単な Streamlit による音声文字起こしアプリの例です。

```python:app.py
import base64
import os

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()  # 環境変数を読み込む

# 環境変数からAPIキーを読み込む
api_key = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=api_key)

st.title("音声文字起こしアプリ")

audio_file = st.file_uploader(
    "音声ファイルをアップロードしてください", type=["m4a", "mp3", "webm", "mp4", "mpga", "wav"]
)

if audio_file is not None:
    st.audio(audio_file, format="audio/wav")

    if st.button("音声文字起こしを実行する"):
        with st.spinner("音声文字起こしを実行中です..."):
            transcript = client.audio.transcriptions.create(
                model="whisper-1", file=audio_file, response_format="text"
            )
        st.success("音声文字起こしが完了しました！")
        st.write(transcript)

        # 文字起こしをバイトに変換し、それをbase64でエンコードする
        transcript_encoded = base64.b64encode(transcript.encode()).decode()
        # ダウンロードリンクを作成する
        st.markdown(
            f'<a href="data:file/txt;base64,{transcript_encoded}" download="transcript.txt">Download Result</a>',
            unsafe_allow_html=True,
        )
```

このコードは、Streamlit で基本的な UI を作成し、音声ファイルをアップロードする機能を提供します。

### ステップ 2: アプリの実行

以下のコマンドで Streamlit アプリを起動します。

```shell
streamlit run app.py
```

これにより、ブラウザ上でアプリが開き、ユーザーは音声ファイルをアップロードして文字起こしを行うことができます。
※ デフォルトの設定（localhost）で実行すると、アプリケーションはローカルマシンからのみアクセス可能となります。安全なローカルネットワーク内で使用してください。

#### 実行画面

![demo](https://storage.googleapis.com/zenn-user-upload/5e89a4e223bc-20231215.gif)

## 補足機能の追加

ここまでで、音声文字起こしアプリはできるのですが、どうせならもう少し汎用性が欲しいと思い以下の機能を追加しました。

追加した機能

- API 上限である 25MB を超えるファイルがアップロードされた場合は、音声を 10 分毎に分割して文字起こしを実施する
- 文字起こしした文章を OpenAI API を使って校正し、句読点や誤字脱字が修正された文字列を最終結果として出力する

```python:app.py
import base64
import io
import os
from logging import DEBUG, StreamHandler, getLogger
from typing import Any, List

import streamlit as st
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI
from pydub import AudioSegment
from pydub.silence import split_on_silence

load_dotenv()  # 環境変数を読み込む
api_key = os.getenv("OPENAI_API_KEY")  # 環境変数からAPIキーを読み込む
client = OpenAI(api_key=api_key)

# ログ出力の設定
logger = getLogger(__name__)
handler = StreamHandler()
handler.setLevel(DEBUG)
logger.setLevel(DEBUG)
logger.addHandler(handler)
logger.propagate = False


class JapaneseCharacterTextSplitter(RecursiveCharacterTextSplitter):
    def __init__(self, **kwargs: Any):
        separators = ["\n\n", "\n", "。", "、", " ", ""]
        super().__init__(separators=separators, **kwargs)


japanese_splitter = JapaneseCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=0,
)


def split_audio(
    file: Any, max_size: int = 600000
) -> list[AudioSegment]:  # API上限は25MBだが、余裕を持たせて20MBに設定
    """
    音声ファイルを指定された最大サイズに分割します。

    Args:
        file (Any): 分割する音声ファイル。
        max_size (int, optional): 分割後の各ファイルの最大サイズ（バイト単位）。デフォルトは25MB。

    Returns:
        Any: 分割された音声ファイル。
    """
    audio = AudioSegment.from_file_using_temporary_files(file)
    chunks = split_on_silence(audio, min_silence_len=2000, silence_thresh=-40)

    combined_chunks = []
    current_chunk = chunks[0]
    for chunk in chunks[1:]:
        if len(current_chunk) + len(chunk) < max_size:
            current_chunk += chunk
        else:
            combined_chunks.append(current_chunk)
            current_chunk = chunk
    combined_chunks.append(current_chunk)

    return combined_chunks


def convert_audio(audio_file: AudioSegment) -> bytes:
    """
    AudioSegment 形式の音声ファイルをバイト形式に変換します。

    Args:
        audio_file (AudioSegment): バイト形式に変換する音声ファイル。

    Returns:
        bytes: バイト形式に変換された音声ファイル。
    """
    byte_io = io.BytesIO()
    audio_file.export(byte_io, format="aac")
    byte_audio = byte_io.getvalue()

    return byte_audio


def extract_text_from_audio(client: OpenAI, audio_file: bytes) -> str:
    """
    音声ファイルからテキストを抽出します。

    Args:
        audio_file (Any): 音声ファイル。

    Returns:
        str: 音声ファイルから抽出されたテキスト。
    """
    try:
        transcript = client.audio.transcriptions.create(
            model="whisper-1", file=audio_file, response_format="text"
        )
    except Exception as e:
        raise e

    return transcript


def correct_japanese_text(client: OpenAI, text: str) -> str:
    """
    日本語のテキストを修正します。

    Args:
        client (OpenAI): 日本語のテキストを修正するためのクライアント。
        text (str): 修正する日本語のテキスト。

    Returns:
        str: 修正された日本語のテキスト。
    """
    try:
        # テキストをチャンクに分割
        chunks = japanese_splitter.split_text(text)

        # 各チャンクを校正
        corrected_chunks = []
        for chunk in chunks:
            prompt = f"##音声文字起こしで不自然な文を修正し、自然な文章にしてください。文章の修正は句読点の追加と誤字脱字の修正にとどめ、要約は絶対にしないでください。\n##音声文字起こし\n{chunk}\n##修正した文章\n"
            messages = [
                {"role": "system", "content": "あなたは優秀な日本語の編集者です。"},
                {"role": "user", "content": prompt},
            ]
            text_modified = client.chat.completions.create(
                model="gpt-3.5-turbo-1106",
                messages=messages,
                temperature=0,
            )
            corrected_chunks.append(text_modified.choices[0].message.content)
    except Exception as e:
        raise e

    # 校正されたチャンクを結合
    return "".join(corrected_chunks)


def main():
    # アプリケーションのタイトルを設定する
    st.title("音声文字起こしアプリ")

    # アップロードダイアログを表示する
    audio_file = st.file_uploader(
        "音声ファイルをアップロードしてください", type=["m4a", "mp3", "webm", "mp4", "mpga", "wav"]
    )

    # アップロードされた音声ファイルのサイズを確認して、25MBを超えていたら、分割する
    if audio_file is not None:
        # アップロードされた音声ファイルの文字起こしを実行する
        if st.button("音声文字起こしを実行する"):
            full_transcript = ""
            if audio_file.size > 25 * 1024 * 1024:
                st.warning("25 MBを超えるファイルがアップロードされました。音声ファイルを分割します")
                with st.spinner("音声ファイルを分割中です..."):
                    audio_segments = split_audio(audio_file)
                    st.info(f"音声ファイルを{len(audio_segments)}個に分割しました")
                    for i, audio_segment in enumerate(audio_segments):
                        st.info(f"{i + 1}個目の音声ファイルを変換中です...")
                        audio_file = convert_audio(audio_segment)
                        st.info(f"{i + 1}個目の音声ファイルの文字起こしを実行中です...")
                        transcript = extract_text_from_audio(client, audio_file)
                        st.info(f"{i + 1}個目の音声ファイルの文字起こしを校正中です...")
                        corrected_transcript = correct_japanese_text(client, transcript)
                        full_transcript += corrected_transcript
                st.success("音声ファイルの分割が完了しました！")
            else:
                st.info("音声ファイルを文字起こし中です...")
                transcript = extract_text_from_audio(client, audio_file)
                st.info("音声ファイルの文字起こしを校正中です...")
                corrected_transcript = correct_japanese_text(client, transcript)
                full_transcript += corrected_transcript

            st.success("全ての音声文字起こしが完了しました！")
            st.write(full_transcript)

            # 文字起こしをバイトに変換し、それをbase64でエンコードする
            transcript_encoded = base64.b64encode(full_transcript.encode()).decode()
            # ダウンロードリンクを作成する
            st.markdown(
                f'<a href="data:file/txt;base64,{transcript_encoded}" download="transcript.txt">Download Result</a>',
                unsafe_allow_html=True,
            )


if __name__ == "__main__":
    main()
```

## まとめ

この記事では、Whisper と Streamlit を使った議事録文字起こしアプリの開発についてご紹介しました。Whisper の高精度な音声認識機能と、Streamlit の直感的なアプリ開発フレームワークが、議事録作成の効率化にどのように貢献するかを見てきました。

まず、Whisper による音声からテキストへの変換の精度の高さは、多言語対応や雑音下での認識能力にも及びます。これにより、様々な環境下での会議やイベントでの利用が可能です。

次に、Streamlit を使ったアプリ開発の手軽さと柔軟性が、技術的な深い知識がなくても迅速なプロトタイピングを実現します。これにより、自分だけのカスタマイズされたアプリを簡単に作成できることが分かりました。

最後に、Langchain と Whisper の組み合わせは、より高度なテキスト処理とアプリケーションの可能性を広げます。Langchain の柔軟性を利用することで、様々なシナリオに応じたテキスト処理が可能になることをお伝えしました。

このように、Whisper と Streamlit を組み合わせることで、議事録作成の時間と労力を大幅に削減し、より効率的で生産的なワークフローを実現することができます。技術的な挑戦に興味がある方にとって、このプロジェクトは新たな可能性を開く一歩となるでしょう。

## 参考記事

https://platform.openai.com/docs/guides/speech-to-text

https://platform.openai.com/docs/guides/text-generation/chat-completions-api

https://zenn.dev/tsuzukia/articles/eb116703b71c5f

https://www.sato-susumu.com/entry/2023/04/30/131338
