---
title: "Cloud Run Job ã‚’ä½¿ã£ã¦ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ã‚’æ¥½ã«å®Ÿè£…ã™ã‚‹"
emoji: "ğŸ˜½"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["cloudrun", "gcp", "batch", "python"]
published: true
published_at: 2024-09-17 08:00 # äºˆç´„æŠ•ç¨¿ï¼ˆæŠ•ç¨¿æ—¥æ™‚ã‚’æŒ‡å®šã™ã‚‹ï¼‰
publication_name: gixo # PublicationæŠ•ç¨¿ã‚’ã™ã‚‹éš›ã®ã¿æŒ‡å®šã™ã‚‹
---

## ã¯ã˜ã‚ã«

ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…ã«ãŠã„ã¦ã€ä¾å­˜é–¢ä¿‚ã®ãªã„å‡¦ç†ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ä¸¦åˆ—åŒ–ã™ã‚‹ã“ã¨ã¯ä¸€èˆ¬çš„ã§ã™ã€‚
ã—ã‹ã—ã€Cloud Run Service ã‚’ä½¿ã£ã¦ã€ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè£…ã™ã‚‹ã¨ãªã‚‹ã¨ã€å®Ÿè£…ãŒè¤‡é›‘ã«ãªã‚ŠãŒã¡ã§ã™ã€‚ãã“ã§ã€ä½¿ã£ãŸã“ã¨ã®ãªã‹ã£ãŸ Cloud Run Job ã‚’ä½¿ã£ã¦ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ã‚’ç°¡å˜ã«å®Ÿè£…ã§ãã‚‹ã‹æ¤œè¨¼ã—ã¦ã¿ã¾ã—ãŸã€‚

## TL;DR

Cloud Run ã«ã¯ã€HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘ä»˜ã‘ã‚‹ Cloud RUn ã‚µãƒ¼ãƒ“ã‚¹ã¨ã€å®šæœŸçš„ãªã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã™ã‚‹ Cloud Run ã‚¸ãƒ§ãƒ– ã® 2 ã¤ã®ãƒ¢ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã¯ã€ãƒãƒƒãƒå‡¦ç†ã®ãƒ¢ãƒ¼ãƒ‰ã§ã‚ã‚‹ Cloud Run Jobs ã‚’ä½¿ã£ã¦ BigQuery ã®ãƒ‡ãƒ¼ã‚¿ã‚’å®šæœŸçš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å‡¦ç†ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚

å˜ç´”ãªãƒãƒƒãƒå‡¦ç†ãªã‚‰ã€ä¸¦åˆ—å‡¦ç†ã®ãƒãƒƒãƒå‡¦ç†ã‚’çµ„ã‚€ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ãŸã ã—ã€ä¸¦åˆ—æ•°ã¨å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãŒä¸€è‡´ã—ã¦ã„ãªã„å ´åˆã€æ€ã£ãŸã‚ˆã†ã«å‡¦ç†ãŒå®Œäº†ã—ãªã„ãŸã‚ã€å®Ÿéš›ã«ãƒãƒƒãƒå‡¦ç†ã‚’çµ„ã‚€å ´åˆã¯å·¥å¤«ãŒå¿…è¦ãã†ã§ã™ã€‚

## Cloud Run Jobs ã¨ã¯

ã¨ã‚Šã‚ãˆãš Cloud Run Jobs ã£ã¦ä½•

|                | Service                                                       | Jobs                                                           |
| -------------- | ------------------------------------------------------------- | -------------------------------------------------------------- |
| å®Ÿè¡Œæ–¹æ³•       | HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒˆãƒªã‚¬ãƒ¼ã¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒŠã‚’å®Ÿè¡Œã™ã‚‹             | ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚„ gcloud ã‚³ãƒãƒ³ãƒ‰ã€Google Cloud APIs çµŒç”±ã§å®Ÿè¡Œã™ã‚‹ |
| å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚° | HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒˆãƒªã‚¬ãƒ¼ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹                       | ä»»æ„ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å®Ÿè¡Œã§ãã‚‹                                   |
| æ©Ÿèƒ½           | ã‚¦ã‚§ãƒ– ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ã‚¤ãƒ™ãƒ³ãƒˆã€é–¢æ•°ã«å¿œç­”ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã«ä½¿ç”¨ | éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã‚‹                   |

è©³ã—ãã¯ã€ã“ã¡ã‚‰ã‚’å‚è€ƒ
https://zenn.dev/google_cloud_jp/articles/cloudrun-jobs-basic

## æ¤œè¨¼ã®æµã‚Œ

### å‡ºåŠ›å…ˆã® Cloud Storage ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹

```bash
gcloud storage buckets create gs://test-cloud-run-jobs \
  --location=us-central1 \
  --default-storage-class=STANDARD \
  --uniform-bucket-level-access
```

### ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆ

```bash
gcloud iam service-accounts create cloud-run-jobs-sa \
  --display-name "Cloud Run Jobs Service Account"
```

### ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«å¿…è¦ãªæ¨©é™ã‚’ä»˜ä¸

å¿…è¦ãªæ¨©é™ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

```bash
gcloud projects add-iam-policy-binding {project_id} \
  --member=serviceAccount:cloud-run-jobs-sa@{project_id}.iam.gserviceaccount.com \
  --role=roles/bigquery.admin
gcloud projects add-iam-policy-binding {project_id}  \
  --member=serviceAccount:cloud-run-jobs-sa@{project_id}.iam.gserviceaccount.com \
  --role=roles/storage.objectAdmin
gcloud projects add-iam-policy-binding {project_id} \
  --member=serviceAccount:cloud-run-jobs-sa@{project_id}.iam.gserviceaccount.com \
  --role=roles/logging.logWriter
gcloud projects add-iam-policy-binding {project_id} \
  --member=serviceAccount:cloud-run-jobs-sa@{project_id}.iam.gserviceaccount.com \
  --role=roles/storage.admin
```

### å®Ÿè£…

BigQuery ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

å¤§ã¾ã‹ãªå‡¦ç†ã®æµã‚Œã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

- BigQuery ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒªã‚¹ãƒˆã‚’å–å¾—
- GCS ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸå‰å›åˆ†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
- BigQuery ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ GCS ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

ä¸¦åˆ—å‡¦ç†ã«ã™ã‚‹ãŸã‚ã«å·¥å¤«ã—ãŸéƒ¨åˆ†ã¯ã€ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚
ã‚¿ã‚¹ã‚¯ã¯ 30 ä¸¦åˆ—ã§å®Ÿè¡Œã—ã¾ã™ãŒã€ãã‚Œãã‚ŒãŒåˆ¥ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†ã™ã‚‹ã‚ˆã†ã«ã€task_index ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
ä¾‹ï¼š `task_index = int(os.environ.get("CLOUD_RUN_TASK_INDEX", 0))`
ãã—ã¦ã€`tables[task_index]` ã§ ãƒªã‚¹ãƒˆã®ã†ã¡ task_index ç•ªç›®ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‡¦ç†ã—ã¦ã„ã¾ã™ã€‚

é›£ç‚¹ã¯ã€ä¸¦åˆ—æ•°ã¨ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãŒä¸€è‡´ã—ã¦ã„ãªã„å ´åˆã€æ€ã£ãŸã‚ˆã†ã«å‡¦ç†ãŒå®Œäº†ã—ãªã„ã“ã¨ã§ã™ã€‚
ä¾‹ãˆã°ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãŒ 10 ã§ä¸¦åˆ—æ•°ãŒ 30 ã®å ´åˆã€20 å€‹ã®ã‚¿ã‚¹ã‚¯ãŒã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™ã€‚
é€†ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãŒ 30 ã§ä¸¦åˆ—æ•°ãŒ 10 ã®å ´åˆã€20 å€‹ã®ã‚¿ã‚¹ã‚¯ãŒä½•ã‚‚ã›ãšã«çµ‚äº†ã—ã¾ã™ã€‚

ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãŒå¤‰æ›´ã«ãªã£ã¦ã‚‚ã€ä¸¦åˆ—æ•°ã‚’å¤‰æ›´ã—ãªãã¦ã‚‚ã„ã„ã‚ˆã†ã«ã€å®Ÿéš›ã«ãƒãƒƒãƒå‡¦ç†ã‚’çµ„ã‚€å ´åˆã¯å·¥å¤«ãŒå¿…è¦ãã†ã§ã™ã€‚

```python: main.py
import os  # noqa: D100

from google.cloud import bigquery, logging, storage  # noqa: D100

# ãƒ­ã‚®ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
logging_client = logging.Client()
logger = logging_client.logger("export-table-to-gcs")


def get_tables(bq_client: bigquery.Client, dataset_id: str) -> list[str]:
    """Get table IDs in the dataset.

    Args:
        bq_client (bigquery.client): BigQuery client
        dataset_id (str): Dataset ID

    Returns:
        list[str]: Table IDs
    """
    try:
        dataset_ref = bq_client.dataset(dataset_id)
        tables = bq_client.list_tables(dataset_ref)
    except Exception as e:
        logger.log_text(f"Failed to list tables in {dataset_id}: {e}", severity="ERROR")
        raise e

    return sorted([table.table_id for table in tables])


def export_table_to_gcs(
    bq_client: bigquery.Client,
    dataset_id: str,
    table_id: str,
    destination_uri: str,
) -> None:
    """Export BigQuery table to GCS.

    Args:
        bq_client (bigquery.client): BigQuery client
        dataset_id (str): Dataset ID
        table_id (str): Table ID
        destination_uri (str): GCS URI
    """
    try:
        dataset_ref = bq_client.dataset(dataset_id)
        table_ref = dataset_ref.table(table_id)
    except Exception as e:
        logger.log_text(f"Failed to get table reference: {e}", severity="ERROR")
        raise e

    try:
        extract_job = bq_client.extract_table(
            table_ref,
            destination_uri,
            location="US",
        )
        extract_job.result()  # å‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
    except Exception as e:
        logger.log_text(f"Failed to extract {table_id}: {e}", severity="ERROR")
        raise e
    logger.log_text(f"Exported {table_id} to {destination_uri}", severity="INFO")


def cleanup_gcs_files(gcs_client: storage.Client, bucket_name: str, table_name: str) -> None:
    """Delete all files in GCS.

    Args:
        gcs_client (storage.Client): GCS client
        bucket_name (str): Bucket name
        table_name (str): Table name
    """
    try:
        bucket = gcs_client.get_bucket(bucket_name)
        blobs = bucket.list_blobs(prefix=f"{table_name}/")
        if not blobs:
            logger.log_text(f"No files in {bucket_name}", severity="INFO")
            return
        for blob in blobs:
            blob.delete()
    except Exception as e:
        logger.log_text(f"Failed to delete files in {bucket_name}: {e}", severity="ERROR")
        raise e
    logger.log_text(f"Deleted all files in {bucket_name}", severity="INFO")


def main() -> str:
    """Export a table to GCS.

    Returns:
        str: A message
    """
    project_id = os.environ.get("PROJECT_ID")
    dataset_id = os.environ.get("SOURCE_DATASET")
    bucket_name = os.environ.get("DESTINATION_BUCKET")

    bq_client = bigquery.Client()
    gcs_client = storage.Client()

    logger.log_text(f"Connected to BigQuery: {bq_client}", severity="INFO")
    print(f"project_id: {project_id}")
    print(f"source_dataset: {dataset_id}")
    print(f"bq_client: {bq_client}")
    tables = get_tables(bq_client, dataset_id)
    logger.log_text(f"Tables in {dataset_id}: {tables}", severity="INFO")

    # TASK_INDEXç’°å¢ƒå¤‰æ•°ã§ã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    task_index = int(os.environ.get("CLOUD_RUN_TASK_INDEX", 0))
    logger.log_text(f"Task index: {task_index}", severity="INFO")

    # ã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
    table_id = tables[task_index]
    # å‰å›ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    cleanup_gcs_files(gcs_client, bucket_name, table_id)
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯1ãƒ•ã‚¡ã‚¤ãƒ«1GBã¾ã§ãªã®ã§ã€è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†å‰²ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    destination_uri = f"gs://{bucket_name}/{table_id}/{table_id}_*.csv"
    export_table_to_gcs(bq_client, dataset_id, table_id, destination_uri)
    logger.log_text(f"Exported {table_id} to {destination_uri}", severity="INFO")

    return f"Exported {table_id} to {destination_uri}"


if __name__ == "__main__":
    main()

```

ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€Github ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚
[test-cloud-run-jobs](https://github.com/y-tsuritani/test-cloud-run-jobs)

### Cloud Run ã«ãƒ‡ãƒ—ãƒ­ã‚¤

```bash
gcloud builds submit ./src --tag gcr.io/{project_id}/test-cloud-run-job

gcloud run jobs create export-bq-to-gcs-job \
  --image=gcr.io/{project_id}/test-cloud-run-job \
  --memory=512Mi \
  --region=us-central1 \
  --service-account=cloud-run-jobs-sa \
  --set-env-vars=PROJECT_ID={project_id},SOURCE_DATASET=TEST_DATA,DESTINATION_BUCKET=test-cloud-run-jobs \
  --tasks 30
```

2 å›ç›®ä»¥é™ã¯ã€`gcloud run jobs create` ã§ã¯ãªã `gcloud run jobs update`ã§æ›´æ–°ã§ãã¾ã™ã€‚

### ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨æ„ã™ã‚‹

ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨æ„ã—ã¾ã™ã€‚ä¸­èº«ã¯ä½•ã§ã‚‚ã„ã„ã§ã™ã€‚
é©å½“ã« csv ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ 30 å€‹ä½œæˆã—ã¦ã€GCS ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
ãã®å¾Œã€csv ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ BigQuery ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

csv ç”¨ã®ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ã¦ãŠãã¾ã™ã€‚

```bash
gsutil mb gs://test-source-csv
```

```bash
cd data
bash create_dummy_tables.sh
```

### Cloud Run Jobs ã‚’å®Ÿè¡Œ

Cloud Run Jobs ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
gcloud run jobs execute export-bq-to-gcs-job --region us-central1
```

å®Ÿè¡Œã«æˆåŠŸã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```bash
app-py3.12tyj app % gcloud run jobs execute export-bq-to-gcs-job --region us-central1
âœ“ Creating execution... Done.
  âœ“ Provisioning resources...
Done.
Execution [export-bq-to-gcs-job-mwxpd] has successfully started running.

View details about this execution by running:
gcloud run jobs executions describe export-bq-to-gcs-job-mwxpd

Or visit https://console.cloud.google.com/run/jobs/executions/details/us-central1/export-bq-to-gcs-job-mwxpd/tasks?
```

å‡ºåŠ›ã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡ŒçŠ¶æ³ã‚’ç¢ºèªã§ãã¾ã™ã€‚
Cloud Run Service ã§ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèªã™ã‚‹ã®ãŒé¢å€’ãªã®ã§ã€Cloud Run Jobs ã‚’ä½¿ã†ã¨ GUI ã§ç°¡å˜ã«ç¢ºèªãŒã§ãã¦ä¾¿åˆ©ãã†ã§ã™ã€‚

![å®Ÿè¡ŒçŠ¶æ³](https://storage.googleapis.com/zenn-user-upload/756be7b66df8-20240916.png)

### ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª

ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ãŸãŒã€ç„¡äº‹ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã—ãŸã€‚
å®Ÿè¡Œå±¥æ­´ã‚’ç¢ºèªã—ã¾ã—ãŸãŒã€ã¡ã‚ƒã‚“ã¨ä¸¦åˆ—ã§å‡¦ç†ã•ã‚Œã¦ãã†ã§ã—ãŸã€‚

### è³‡æã‚’å‰Šé™¤

æ¤œè¨¼ãŒçµ‚ã‚ã£ãŸã‚‰ã€å¿˜ã‚Œãšã«è³‡æã‚’å‰Šé™¤ã—ã¾ã™ã€‚

### Next Step

ä»Šå›ã®æ¤œè¨¼ã«ã¯å«ã‚ã¾ã›ã‚“ã§ã—ãŸãŒã€ã‚‚ã†å°‘ã—è¤‡é›‘ãªãƒãƒƒãƒå‡¦ç†ã‚’çµ„ã‚€å ´åˆã¯ã€Workflows ã‹ã‚‰ Cloud Run Jobs ã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
`googleapis.run.v1.namespaces.jobs.run` ã§ Cloud Run Jobs ã‚’å‘¼ã³å‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚
å®Ÿè¡Œæ™‚ã«ã¯ç’°å¢ƒå¤‰æ•°ã®ä¸Šæ›¸ããŒã§ãã‚‹ãŸã‚ã€ãƒ†ãƒ¼ãƒ–ãƒ«åã‚„ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå…ˆã®ãƒã‚±ãƒƒãƒˆåã‚’å‹•çš„ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://cloud.google.com/workflows/docs/tutorials/execute-cloud-run-jobs?hl=ja#deploy-workflow)

### ãŠã‚ã‚Šã«

Cloud Run Jobs ã‚’ä½¿ã£ã¦ BigQuery ã®ãƒ‡ãƒ¼ã‚¿ã‚’å®šæœŸçš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒãƒƒãƒå‡¦ç†ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚
å˜ç´”ãªãƒãƒƒãƒå‡¦ç†ãªã‚‰ã€ä¸¦åˆ—å‡¦ç†ã®ãƒãƒƒãƒå‡¦ç†ã‚’çµ„ã‚€ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ãŸã ã—ã€ä¸¦åˆ—æ•°ã¨å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°ãŒä¸€è‡´ã—ã¦ã„ãªã„å ´åˆã€æ€ã£ãŸã‚ˆã†ã«å‡¦ç†ãŒå®Œäº†ã—ãªã„ãŸã‚ã€å®Ÿéš›ã«ãƒãƒƒãƒå‡¦ç†ã‚’çµ„ã‚€å ´åˆã¯å·¥å¤«ãŒå¿…è¦ãã†ãªã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚
ä¸¦åˆ—å‡¦ç†ã¯ã€ãƒãƒƒãƒå‡¦ç†ã‚’è¨­è¨ˆã™ã‚‹ä¸Šã§é‡è¦ãªè¦ç´ ãªã®ã§ã€ä»Šå¾Œã‚‚æ¤œè¨¼ã‚’ç¶šã‘ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚
